# Python program to 
# demonstrate implementation of 
# queue using queue module 
  
  
from multiprocessing import Process, Queue, Pool
from queue import Empty as qEmpty  # This is messy
from time import sleep
from twittertools.tweet_import import tweet_import
from twittertools.make_word_cloud import word_cloud_from_txt
from copy import deepcopy
from os.path import isfile
from sys import stderr
from ffmpegencode import ffmpegconverter
import re
# Used to save the twitter instances for distributed processing
'''
    Example of how to wait for enqueued tasks to be completed:

    def worker():
        while True:
            item = q.get()
            if item is None:
                break
            do_work(item)
            q.task_done()

    q = queue.Queue()
    threads = []
    for i in range(num_worker_threads):
        t = threading.Thread(target=worker)
        t.start()
        threads.append(t)

    for item in source():
        q.put(item)

    # block until all tasks are done
    q.join()

    # stop workers
    for i in range(num_worker_threads):
        q.put(None)
    for t in threads:
        t.join()
'''

def do_twitter_analysis(tweet_obj):  
    '''
    Perform the time consuming analysis steps done after the twitter API

    This involves working with images and Google Vision
    ''' 
    #, image_files=[], summary_file=[]):  # do_work
    # print(f'working ...')
    print(f'--Woring {tweet_obj.curFolder}--')
    tweet_obj.classify_images()  # Makes requests to Google Vision
    outfile = tweet_obj.write_summaryfile()  # Combines tweets, labels into summary
    if not isfile(outfile):
        print(f'\n\n--output file ({outfile}) not found--')  # , file=stderr)
    else:
        word_cloud_from_txt(outfile)  # 
    # print(f'done')

def work_twitterdata(q):  # worker
    # print('Initiating work')
    '''
    while True:
        try:
            print('Trying to get item')
            item = q.get(block=True,timeout=2)
        except queue.Empty:
            print('passing on empty queue')
            pass
        else:
            print(f'Recieved: {item}')
            do_twitter_analysis(item)
            q.task_done()
    '''
    item = q.get()
    if item is None:
        return
    do_twitter_analysis(item)



'''
    This provides a full interface to summarize a users twitter feed
    -- Currently looks at both text (tweets, retweets, replies) and images
    -- -- images are summarized by labels generated by Google Vision

    May be interesting to allow specific dates and so fourth eventually
'''
class queue_twitter_summary():
    '''
    Provides an interface to download twitter data
    '''
    def __init__(self):

        maxque = 10                  # Test values
        username = 'potus'           # Test values
        pages = 10                   # Test values

        tweetClass = tweet_import() # Connect to twitter
        # q = Queue(maxsize=maxque) 
        # q = Queue() 
        # the_pool = Pool(4, work_twitterdata,(q,))
        the_pool = Pool(10)
        tweetobjs = []
        for i in range(pages):
            print(f'\nAnalyzing page {i+1} of {(pages)}\n')
            tweetClass.analyzeUsername(username, tweetcount=10)
            tweetobjs.append(deepcopy(tweetClass))
            # q.put(tweetobjs[-1])
        # q.close()
        # q.join_thread()
        the_pool.map(do_twitter_analysis, tweetobjs)
        the_pool.close()
        the_pool.join()
        videocreator = ffmpegconverter()
        outdir = re.sub('_iter\\d+', '*/twitter_*.png', tweetobjs[0].curFolder)
        print(outdir)
        ff = ffmpegconverter()
        ff.twitter_to_mpeg4(file_pattern=outdir)
        # the_pool.close()
        # the_pool.join()

        # # If we go more than 30 seconds without something, die
        #     try:
        #         T = 60
        #         print(f"Waiting for item from queue for up to {T} seconds")
        #         i = q.get(True, T)
        #         print(f'found {i} from the queue !!')
        #         its.append(i)
        #     except qEmpty:
        #         print("Caught queue empty exception, done")
        #         break
        '''
        for imageFiles in imagesets:
            print(*imageFiles, sep='\n', end='[end]\n')
        print('\nsummaries\n')
        print(*summaries, sep='\n')
        if len(summaries) != len(imagesets):
            print(f'Expected a list of length {pages} (pages) for'+ \
                  f'summaries ({len(summaries)}) and images ({len(imagesets)})',
                  file=stderr)
            raise Exception
        # '''
        # for obj in tweetobjs:
        #     do_twitter_analysis(obj)
            # q.put(tweet, block=True, timeout=1)
        # print('items placed')
        # # block until all tasks are done
        # print('joining items')
        # q.join()
        # print('queue joined')

        # # stop workers
        # for i in range(len(tweets)):
        #     q.put(None)
        # print('put none')
        # for t in threads:
        #     t.join()
        # print('threads joined')


'''
        # tweetClass.classify_images()
        # word_cloud_from_txt(tweetClass.write_summaryfile())
'''
if __name__ == '__main__':
    a = queue_twitter_summary()
