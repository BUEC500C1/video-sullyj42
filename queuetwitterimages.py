# Python program to 
# demonstrate implementation of 
# queue using queue module 
  
  
import queue 
import threading
from time import sleep
from twittertools.tweet_import import tweet_import
from twittertools.make_word_cloud import word_cloud_from_txt
from copy import deepcopy
from os.path import isfile
from sys import stderr
# Used to save the twitter instances for distributed processing
'''
    Example of how to wait for enqueued tasks to be completed:

    def worker():
        while True:
            item = q.get()
            if item is None:
                break
            do_work(item)
            q.task_done()

    q = queue.Queue()
    threads = []
    for i in range(num_worker_threads):
        t = threading.Thread(target=worker)
        t.start()
        threads.append(t)

    for item in source():
        q.put(item)

    # block until all tasks are done
    q.join()

    # stop workers
    for i in range(num_worker_threads):
        q.put(None)
    for t in threads:
        t.join()
'''

def do_twitter_analysis(tweet_obj, image_files, summary_file):  # do_work
    # print(f'working ...')
    labels = tweet_obj.classify_images(images=image_files)  # Makes requests to Google Vision
    print('classified images and obtained labels')
    print(f'Generated a set of labels: {labels}', sep=', ', end='[end]\n')
    outfile = tweet_obj.write_summaryfile(summary_file)  # Combines tweets, labels into summary
    if not isfile(outfile):
        print(f'output file not properly created: {outfile}')
        return
    else:
        word_cloud_from_txt(outfile)  # 
    # print(f'done')

def work_twitterdata(q):  # worker
    # print('Initiating work')
    while True:
        try:
            print('Trying to get item')
            item = q.get(block=True,timeout=2)
        except queue.Empty:
            print('passing on empty queue')
            pass
        else:
            print(f'Recieved: {item}')
            do_twitter_analysis(item)
            q.task_done()



'''
    This provides a full interface to summarize a users twitter feed
    -- Currently looks at both text (tweets, retweets, replies) and images
    -- -- images are summarized by labels generated by Google Vision

    May be interesting to allow specific dates and so fourth eventually
'''
class queue_twitter_summary():
    '''
    Provides an interface to download twitter data
    '''
    def __init__(self):

        maxque = 5
        q = queue.Queue() 
        threads = []
        username = 'brabbott42'
        pages = 5

        tweetClass = tweet_import()
        # a.analyzeUsername('brabbott42', range(0, 1000, 200))
        imagesets = []
        summaries = []
        for i in range(pages):
            temp_images, temp_summaries = tweetClass.analyzeUsername(username, tweetcount=10)
            summaries.append(temp_summaries)
            imagesets.append(temp_images)
            # imagesets.append(output.image_files)
            # summaries.append(output.clean_tweet_file)
            # This updates the page number incrementally
        print('Tweets retrieved')
        '''
        # Iterative approach (no threads required)
        for tweet in tweets:
            do_twitter_analysis()
        '''
        threads = []
        # for i in range(2):
        #     t = threading.Thread(target=work_twitterdata, args=(q,))
        #     t.start()
        #     threads.append(t)
        # print('Threads created, printing tweet states')
        for imageFiles in imagesets:
            print(*imageFiles, sep='\n', end='[end]\n')
        print('\nsummaries\n')
        print(*summaries, sep='\n')
        if len(summaries) != len(imagesets):
            print(f'Expected a list of length {pages} (pages) for'+ \
                  f'summaries ({len(summaries)}) and images ({len(imagesets)})',
                  file=stderr)
            raise Exception
        tweetClass.image_files = []
        for i in range(len(imagesets)):
            do_twitter_analysis(tweetClass, imagesets[i], summaries[i])
            # q.put(tweet, block=True, timeout=1)
        # print('items placed')
        # # block until all tasks are done
        # print('joining items')
        # q.join()
        # print('queue joined')

        # # stop workers
        # for i in range(len(tweets)):
        #     q.put(None)
        # print('put none')
        # for t in threads:
        #     t.join()
        # print('threads joined')


'''
        # tweetClass.classify_images()
        # word_cloud_from_txt(tweetClass.write_summaryfile())
'''
if __name__ == '__main__':
    a = queue_twitter_summary()
